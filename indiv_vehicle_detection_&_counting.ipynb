{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "indiv vehicle detection & counting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uB00AY9cQL8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mask Image\n"
      ],
      "metadata": {
        "id": "62FKhspmP7lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "from PIL import Image\n",
        "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
      ],
      "metadata": {
        "id": "CfcVSAoFSXnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataDir='content'\n",
        "dataType='val2017'\n",
        "annFile='/{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
        "\n",
        "annFile = '/content/drive/MyDrive/filtered.json'"
      ],
      "metadata": {
        "id": "GAEgNJ2FVBuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize COCO api for instance annotations\n",
        "coco=COCO(annFile)\n",
        "catIDs = coco.getCatIds()\n",
        "cats = coco.loadCats(catIDs)\n",
        "print(cats)"
      ],
      "metadata": {
        "id": "pYWG41xsVEPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgIds = coco.getImgIds()\n",
        "images = coco.loadImgs(imgIds)\n",
        "print(len(images))"
      ],
      "metadata": {
        "id": "4brLsrcWVGTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(images)):\n",
        "  img = images[i]\n",
        "  filename = '/content/{}/{}'.format(dataType,img['file_name'])\n",
        "  print(img['file_name'])\n",
        "  I = io.imread(filename)\n",
        "  imgId = img['id']\n",
        "  plt.imshow(I)\n",
        "  plt.axis('off')\n",
        "  annIds = coco.getAnnIds(imgIds=imgId, iscrowd=None)\n",
        "  anns = coco.loadAnns(annIds)\n",
        "  coco.showAnns(anns)\n",
        "  mask = np.zeros((img['height'],img['width']),dtype=np.uint8)\n",
        "  for i in range(len(anns)):\n",
        "    print(anns[i])\n",
        "    mask = np.maximum(mask,coco.annToMask(anns[i])*(i+1))\n",
        "    #np.savetxt('test.txt',mask)\n",
        "  data = Image.fromarray(mask)\n",
        "  imgName = img['file_name']\n",
        "  imgName = imgName.replace('.jpg','')\n",
        "  savepath = imgName\n",
        "  print(savepath)\n",
        "  data.save('/content/drive/MyDrive/vehicleDataset/vehicleMasks/{}_mask.png'.format(savepath))"
      ],
      "metadata": {
        "id": "64ihOWtoVaXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "a5lnUr7kP_yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO"
      ],
      "metadata": {
        "id": "42tl0RWaT7nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VehicleDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transforms=None):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    # load all image files, sorting them to\n",
        "    # ensure that they are aligned\n",
        "    self.imgs = list(sorted(os.listdir(os.path.join(root , \"JPGImages\"))))\n",
        "  \n",
        "    self.masks = list(sorted(os.listdir(os.path.join(root, \"vehicleMasks\"))))\n",
        "    #addtional\n",
        "    self.coco = COCO('/content/drive/MyDrive/filtered.json')\n",
        "    imgIds = self.coco.getImgIds()\n",
        "    sort_imgIds = sorted(imgIds)\n",
        "    sort_imgIds.remove(52412)\n",
        "    sort_imgIds.remove(183246)\n",
        "    sort_imgIds.remove(210273)\n",
        "    sort_imgIds.remove(344888)\n",
        "    sort_imgIds.remove(336232)\n",
        "    sort_imgIds.remove(426372)\n",
        "    sort_imgIds.remove(460147)\n",
        "    self.sort_imgIds = sort_imgIds \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # load images ad masks\n",
        "    img_path = os.path.join(self.root, \"JPGImages\", self.imgs[idx])\n",
        "    mask_path = os.path.join(self.root, \"vehicleMasks\", self.masks[idx])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    \n",
        "    annIds = self.coco.getAnnIds(imgIds=self.sort_imgIds[idx])\n",
        "    anns = self.coco.loadAnns(annIds)\n",
        "    \n",
        "    # note that we haven't converted the mask to RGB,\n",
        "    # because each color corresponds to a different instance\n",
        "    # with 0 being background\n",
        "    mask = Image.open(mask_path)\n",
        "    mask = np.array(mask)\n",
        "    # instances are encoded as different colors        \n",
        "    obj_ids = np.unique(mask)\n",
        "    # first id is the background, so remove it\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # split the color-encoded mask into a set\n",
        "    # of binary masks\n",
        "    masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "    # get bounding box coordinates for each mask\n",
        "    num_objs = len(obj_ids)\n",
        "    boxes = []\n",
        "    iscrowd = []\n",
        "    labels = []\n",
        "    \n",
        "    #for i in range(num_objs):\n",
        "    j=0\n",
        "    for i in obj_ids:\n",
        "      labels.append(anns[i-1]['category_id'])\n",
        "      iscrowd.append(anns[i-1]['iscrowd'])\n",
        "      pos = np.where(masks[j])\n",
        "      xmin = np.min(pos[1])\n",
        "      xmax = np.max(pos[1])\n",
        "      ymin = np.min(pos[0])\n",
        "      ymax = np.max(pos[0])\n",
        "      boxes.append([xmin, ymin, xmax, ymax])\n",
        "      j+=1\n",
        "    \n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "      \n",
        "    masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "    image_id = torch.tensor([idx])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "   \n",
        "      \n",
        "    iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
        "    #iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "    #labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "      \n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"masks\"] = masks\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      img, target = self.transforms(img, target)\n",
        "\n",
        "    return img, target"
      ],
      "metadata": {
        "id": "BYA9rvdrQBfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "      \n",
        "def build_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # Stop here if you are fine-tunning Faster-RCNN\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9hXXl-QlSkSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pytorch/vision.git\n",
        "%cd vision\n",
        "!git checkout v0.3.0\n",
        "\n",
        "!cp references/detection/utils.py ../\n",
        "!cp references/detection/transforms.py ../\n",
        "!cp references/detection/coco_eval.py ../\n",
        "!cp references/detection/engine.py ../\n",
        "!cp references/detection/coco_utils.py ../"
      ],
      "metadata": {
        "id": "8Ci4orRqSta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = VehicleDataset('/content/drive/MyDrive/vehicleDataset', get_transform(train=True))\n",
        "dataset_test = VehicleDataset('/content/drive/MyDrive/vehicleDataset', get_transform(train=False))\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "metadata": {
        "id": "Jjc2V9mcUBhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 5\n",
        "\n",
        "# get the model using our helper function\n",
        "model = build_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "metadata": {
        "id": "Z3DQ5bdfUDWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "metadata": {
        "id": "qZqc79tJUFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'mask-rcnn-vehicle2.pt')"
      ],
      "metadata": {
        "id": "O-pxk9Y_UKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##apply model"
      ],
      "metadata": {
        "id": "8vLWvchzQB4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "-uPnvVT3QDsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/mask-rcnn-vehicle2.pt'\n",
        "model = torch.load(PATH)\n",
        "model.eval()\n",
        "CLASS_NAMES = ['__background__', 'car','motorcycle','bus','truck']\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Exw2Pt2lQe7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centroid(x1, y1, x2, y2):\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    \n",
        "    cx = int(x1 + w//2)\n",
        "    cy = int(y1 + h//2)\n",
        "    return cx,cy\n",
        "\n",
        "def get_coloured_mask(mask):\n",
        "    \"\"\"\n",
        "    random_colour_masks\n",
        "      parameters:\n",
        "        - image - predicted masks\n",
        "      method:\n",
        "        - the masks of each predicted object is given random colour for visualization\n",
        "    \"\"\"\n",
        "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    r[mask == 1], g[mask == 1], b[mask == 1] = colours[random.randrange(0,10)]\n",
        "    coloured_mask = np.stack([r, g, b], axis=2)\n",
        "    return coloured_mask\n",
        "\n",
        "def get_prediction(img, confidence):\n",
        "    \"\"\"\n",
        "    get_prediction\n",
        "      parameters:\n",
        "        - img_path - path of the input image\n",
        "        - confidence - threshold to keep the prediction or not\n",
        "      method:\n",
        "        - Image is obtained from the image path\n",
        "        - the image is converted to image tensor using PyTorch's Transforms\n",
        "        - image is passed through the model to get the predictions\n",
        "        - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
        "          ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
        "    \n",
        "    \"\"\"\n",
        "    #img = Image.open(img_path)\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    img = transform(img)\n",
        "\n",
        "    img = img.to(device)\n",
        "    pred = model([img])\n",
        "    pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
        "    print(pred_score)\n",
        "    pred_t = []\n",
        "    \n",
        "    for x in pred_score:\n",
        "      if x > confidence:\n",
        "        pred_t.append(pred_score.index(x))\n",
        "    if len(pred_t) != 0:\n",
        "      pred_t = pred_t[-1]\n",
        "    else:\n",
        "      pred_t = -1\n",
        "\n",
        "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
        "    # print(pred[0]['labels'].numpy().max())\n",
        "    pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())]\n",
        "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
        "    masks = masks[:pred_t+1]\n",
        "    pred_boxes = pred_boxes[:pred_t+1]\n",
        "    pred_class = pred_class[:pred_t+1]\n",
        "    pred_s = pred_score[:pred_t+1]\n",
        "    print(pred_s)\n",
        "    print(pred_class)\n",
        "    return masks, pred_boxes, pred_class,pred_s\n",
        "\n",
        "def segment_instance(img, confidence=0.5, rect_th=2, text_size=1, text_th=2):\n",
        "    \"\"\"\n",
        "    segment_instance\n",
        "      parameters:\n",
        "        - img_path - path to input image\n",
        "        - confidence- confidence to keep the prediction or not\n",
        "        - rect_th - rect thickness\n",
        "        - text_size\n",
        "        - text_th - text thickness\n",
        "      method:\n",
        "        - prediction is obtained by get_prediction\n",
        "        - each mask is given random color\n",
        "        - each mask is added to the image in the ration 1:0.8 with opencv\n",
        "        - final output is displayed\n",
        "    \"\"\"\n",
        "    masks, boxes, pred_cls , pred_s = get_prediction(img, confidence)\n",
        "    offset = 10\n",
        "    matches = []\n",
        "    global num_obj\n",
        "    global num_car\n",
        "    global num_motorcycle\n",
        "    global num_bus\n",
        "    global num_truck\n",
        "    line_height = 120\n",
        "    line_width = 300\n",
        "    height, width, channels = img.shape\n",
        "    print('height',height,width)\n",
        "    #img = cv2.imread(img_path)\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    #print('num_mask=',len(masks))\n",
        "    for i in range(len(masks)):\n",
        "      rgb_mask = get_coloured_mask(masks[i])\n",
        "      img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
        "      x1, y1 = boxes[i][0][0],boxes[i][0][1]\n",
        "      x2, y2 = boxes[i][1][0],boxes[i][1][1]\n",
        "      #print(boxes[i])\n",
        "      centroid = get_centroid(x1,y1,x2,y2)\n",
        "      obj_class = pred_cls[i]\n",
        "      matches.append((centroid,obj_class))\n",
        "      \n",
        "      cv2.circle(img,centroid, 3, (0,0,255), -1)\n",
        "\n",
        "      #vertical line\n",
        "      cv2.line(img, (line_width,0), (line_width, height), (255,255,0), 6)\n",
        "\n",
        "      #horizontal line\n",
        "      #cv2.line(img,(0,line_height),(width,line_height),(255,0,255),6)\n",
        "      \n",
        "      cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n",
        "      cv2.putText(img,pred_cls[i]+' '+\"{:.2f}\".format((pred_s[i])), boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
        "    #print('num_matches=',len(matches))\n",
        "    \n",
        "    for (x,y),label in matches:\n",
        "      #print((x,y),label)\n",
        "        \n",
        "      if (x<line_width+offset) and (x>line_width-offset) and (y>line_height):\n",
        "        cv2.line(img, (line_width,0), (line_width, height), (0,100,255), 6)\n",
        "        num_obj+=1\n",
        "        if (label == 'car'):\n",
        "          num_car += 1\n",
        "        elif (label == 'motorcycle'):\n",
        "          num_motorcycle += 1\n",
        "        elif (label == 'bus'):\n",
        "          num_bus += 1\n",
        "        else:\n",
        "          num_truck += 1\n",
        "        #matches.remove(((x,y),label))\n",
        "        #print(((x,y),label))\n",
        "    print('total_obj=',num_obj)\n",
        "    cv2.putText(img, \"Total Objs Detected: \" + str(num_obj), (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                    (0, 170, 0), 2)\n",
        "    \n",
        "    cv2.putText(img, \"Total Cars Detected: \" + str(num_car), (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                    (0, 170, 0), 2)\n",
        "    \n",
        "    cv2.putText(img, \"Total Motorcycles Detected: \" + str(num_motorcycle), (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                    (0, 170, 0), 2)\n",
        "    \n",
        "    cv2.putText(img, \"Total Buses Detected: \" + str(num_bus), (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                    (0, 170, 0), 2)\n",
        "    \n",
        "    cv2.putText(img, \"Total Trucks Detected: \" + str(num_truck), (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                    (0, 170, 0), 2)\n",
        "    '''\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.imshow(img)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "    '''\n",
        "    return img"
      ],
      "metadata": {
        "id": "U-v3A47TUXIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_STREAM = \"/content/drive/MyDrive/traffic_day.mp4\"\n",
        "VIDEO_STREAM_OUT = \"/content/drive/MyDrive/test_day6.mp4\"\n",
        "\n",
        "vs = cv2.VideoCapture(VIDEO_STREAM)\n",
        "writer = None\n",
        "\n",
        "#vs.set(cv.CAP_PROP_FPS, 25);\n",
        "num_obj=0\n",
        "num_car=0\n",
        "num_motorcycle = 0\n",
        "num_bus = 0\n",
        "num_truck = 0\n",
        "i = 0\n",
        "while (True):\n",
        "  print(i)\n",
        "  # read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "  i += 1\n",
        "   \n",
        "  # If the frame was not grabbed, then we have reached the end\n",
        "  # of the stream\n",
        "  if not grabbed:\n",
        "    print (\"Not grabbed.\")\n",
        "    break;\n",
        "    \n",
        "  \n",
        "  masked_frame = segment_instance(frame, confidence=0.7)\n",
        "  \n",
        "  # Check if the video writer is None\n",
        "  if writer is None:\n",
        "    # Initialize our video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
        "    writer = cv2.VideoWriter(VIDEO_STREAM_OUT, fourcc, 30,\n",
        "      (masked_frame.shape[1], masked_frame.shape[0]), True)\n",
        "   \n",
        "  # Write the output frame to disk\n",
        "  writer.write(masked_frame)\n",
        "  \n",
        "# Release the file pointers\n",
        "print(\"[INFO] cleaning up...\")\n",
        "writer.release() "
      ],
      "metadata": {
        "id": "GOUI6mJgUZ2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}